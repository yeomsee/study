{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# ANN Basic"],"metadata":{"id":"EsRTvu3xtG6H"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"KY1m3Xjxq-jY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728035254427,"user_tz":-540,"elapsed":3032,"user":{"displayName":"염시형","userId":"10102578423005698819"}},"outputId":"c7eca3c2-97bf-4235-f409-8f5c97434d71"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","''' NN\n","    pytorch에서 Neural Net을 디자인하는 모듈\n","'''\n","\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"8ag_6NnQq_1j","executionInfo":{"status":"ok","timestamp":1728035261373,"user_tz":-540,"elapsed":6948,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def load_dataset(file, device):\n","    ''' device\n","        'cuda', 혹은 'cpu'가 넘어오게 됨\n","        -> 'cuda'가 넘어오게 되면 gpu(graphic processing unit)에서 실행\n","        -> 그래픽 내부에서는 주로 행렬 연산이 이루어지기 때문에 gpu를 사용하게 되면 연산이 빨라지고 학습이 빨라짐\n","    '''\n","\n","    data = np.loadtxt(file)\n","    print(\"DATA = \", data)\n","\n","    ''' np.loadtxt\n","        np.array의 형태로 data를 읽어들임\n","    '''\n","\n","    # feature, label 분리\n","    input_features = data[:, :-1]\n","    print(\"input features = \", input_features)\n","\n","    labels = np.reshape(data[:, -1], (4, 1))\n","    print(\"labels = \", labels)\n","\n","    ''' label 형성 과정\n","        1) data[:, -1] : 1행 4열의 구조, 즉 우리가 알던 행벡터의 transpose 구조가 됨\n","        2) np.reshape를 통해서 4행 1열의 구조로 바꾸고자 함\n","    '''\n","\n","    input_features = torch.tensor(input_features, dtype=torch.float).to(device)\n","    labels = torch.tensor(labels, dtype=torch.float).to(device)\n","\n","    ''' torch.tensor\n","        pytorch가 내부적으로 쓰는 자료구조는 array가 아닌 tensor 형태임\n","        tensor는 쉽게 생각해서 차원이 큰 array라고 생각하면 될 듯\n","    '''\n","\n","    ''' torch.tensor.to(device)\n","        device는 위에서 얘기했던 cpu를 사용할거냐, gpu(cuda)를 사용할거냐를 나타내는 놈\n","        이 때, to(device)를 사용해 tensor를 'device'에 올리는 것임\n","    '''\n","\n","    return (input_features, labels) # 여기서 input_features 및 labels는 tensor임에 주의!\n","\n","# tensor는 보기 힘드니까 list의 형태로 보고 싶어!\n","def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()"],"metadata":{"id":"W4xxBgJgrN4f","executionInfo":{"status":"ok","timestamp":1728035261373,"user_tz":-540,"elapsed":7,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    device = 'cuda'\n","else:\n","    device = 'cpu'\n","\n","''' torch.cuda.is_available()\n","    gpu 사용 가능 여부 check\n","'''\n","\n","# load dataset\n","input_features, labels = load_dataset(\"/content/drive/MyDrive/연구실/기계학습/train.txt\", device)\n","\n","# NN model 디자인\n","model = nn.Sequential(\n","    nn.Linear(2, 2, bias=True), nn.Sigmoid(),\n","    nn.Linear(2, 1, bias=True), nn.Sigmoid()).to(device)\n","\n","''' nn.Linear(m, n, bias=True)\n","    m*n 크기의 가중치를 바탕으로 linear하게 연결된 구조\n","'''\n","\n","''' nn.Sigmoid()\n","    nn.Linear를 타고온 값에 sigmoid 함수를 씌우는 역할\n","'''\n","\n","# binary classification : cross entropy cost function\n","loss_func = torch.nn.BCELoss().to(device)\n","\n","# optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","\n","''' optimizer\n","    -> 학습하는 함수, 즉 역전파 알고리즘을 뭘로 사용할 것인가?\n","\n","    torch.optim.SGD(model.parameters(), lr=1)\n","    - SGD : 위 optimizer는 gradient descent 방법을 사용하는데, 그 중에서도 SGD(Stochastic Gradient Descent) 방법을 사용한다고 보면 됨\n","    - model.paramters() : optimizie 할 대상 (위 예시에서는 가중치행렬인 W, 편향인 b가 모델의 파라미터가 된다)\n","    - lr : learning rate\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"id":"iR3pLKMurXb2","executionInfo":{"status":"ok","timestamp":1728035265399,"user_tz":-540,"elapsed":4032,"user":{"displayName":"염시형","userId":"10102578423005698819"}},"outputId":"a5ab2dc5-76a2-40a5-8685-0d5f14086b04"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["DATA =  [[0. 0. 0.]\n"," [0. 1. 1.]\n"," [1. 0. 1.]\n"," [1. 1. 0.]]\n","input features =  [[0. 0.]\n"," [0. 1.]\n"," [1. 0.]\n"," [1. 1.]]\n","labels =  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]]\n"]},{"output_type":"execute_result","data":{"text/plain":["' optimizer\\n    -> 학습하는 함수, 즉 역전파 알고리즘을 뭘로 사용할 것인가?\\n\\n    torch.optim.SGD(model.parameters(), lr=1)\\n    - SGD : 위 optimizer는 gradient descent 방법을 사용하는데, 그 중에서도 SGD(Stochastic Gradient Descent) 방법을 사용한다고 보면 됨\\n    - model.paramters() : optimizie 할 대상 (위 예시에서는 가중치행렬인 W, 편향인 b가 모델의 파라미터가 된다)\\n    - lr : learning rate\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# train mode로 작동하도록 설정\n","model.train()\n","\n","# 모델 학습\n","for epoch in range(1001):\n","    ''' epoch\n","        모든 data를 한 번 다 읽은 것을 1 epoch으로 말함\n","        때문에 모든 데이터를 1000번 읽어서 천천히 수렴해나가겠다라는 뜻\n","    '''\n","\n","    # optimizer 초기화\n","    optimizer.zero_grad()\n","\n","    ''' optimizer.zero_grad()\n","        - zero_grad() : 역전파를 수행하기 전에 기울기를 0으로 초기화\n","\n","        원래 pytorch는 기울기를 누적하는 방식으로 작동함\n","        -> 기울기를 새로 계산할 때마다 이전에 계산된 기울기 값에 더해지는 방식 ... 미니 배치나 여러 배치를 사용하여 손실을 계산할 때 유용함\n","\n","        -> RNN 학습 때는 zero_grad를 하지 않음\n","\n","        하지만! 기본적으로 매 학습 단계마다 기울기를 새로 계산하고, 그 이전 기울기는 사용하지 않음. 때문에 초기화한다.\n","    '''\n","\n","    # 모델 설정 후\n","    hypothesis = model(input_features)\n","\n","    '''\n","        Hypothesis는 그냥 model이라고 생각하면 편하겠다.\n","    '''\n","\n","    # cost를 계산하고\n","    cost = loss_func(hypothesis, labels)\n","\n","    # 역전파 : 손실함수로부터 기울기를 '계산' -> 파라미터 업데이트는 아직 이루어지지 않음!\n","    cost.backward()\n","\n","    '''\n","        참고로 cost.backward()를 통해 기울기를 구하면, 이 값은 파라미터의 .grad로 저장됨\n","    '''\n","\n","    # optimizer 작동시켜 역전파를 실행함 -> 파라미터 업데이트\n","    optimizer.step()\n","\n","    if epoch%100 == 0:\n","        print(epoch, cost.item())\n","\n","    ''' cost.item을 통해 cost값을 불러올 수 있음\n","    '''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0t0sLL3xsey","executionInfo":{"status":"ok","timestamp":1728035266277,"user_tz":-540,"elapsed":885,"user":{"displayName":"염시형","userId":"10102578423005698819"}},"outputId":"5ea073ca-f540-48f0-e6d0-f39bb2f931a4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.7235664129257202\n","100 0.6926396489143372\n","200 0.6908921003341675\n","300 0.6803392767906189\n","400 0.6224880218505859\n","500 0.5384353399276733\n","600 0.43271487951278687\n","700 0.2291935682296753\n","800 0.12024886906147003\n","900 0.0768842101097107\n","1000 0.055503424257040024\n"]}]},{"cell_type":"code","source":["# 평가 모드 설정\n","model.eval()\n","\n","with torch.no_grad():\n","\n","    ''' torch.no_grad()\n","        학습이 아닌 평가를 하는 것이기 때문에 gradient 적용 필요 X\n","    '''\n","\n","    hypothesis = model(input_features)\n","    logits = (hypothesis > 0.5).float()\n","    predicts = tensor2list(logits)\n","    golds = tensor2list(labels)\n","\n","    print(\"pred = \", predicts)\n","    print(\"gold = \", golds)\n","    print(\"Accuracy = {0:f}\".format(accuracy_score(golds, predicts)))"],"metadata":{"id":"GdhQc_Uf4Q-N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728035266278,"user_tz":-540,"elapsed":9,"user":{"displayName":"염시형","userId":"10102578423005698819"}},"outputId":"aec07b7e-564e-425d-baec-18aaf0fc54ba"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["pred =  [[0.0], [1.0], [1.0], [0.0]]\n","gold =  [[0.0], [1.0], [1.0], [0.0]]\n","Accuracy = 1.000000\n"]}]},{"cell_type":"markdown","source":["\n","---"],"metadata":{"id":"Z1PBd6fds_EC"}},{"cell_type":"markdown","source":["# Wide ANN"],"metadata":{"id":"gTM45z1btBqk"}},{"cell_type":"markdown","source":["hidden layer를 2\\*2에서 2\\*10으로 변경  \n","**Widening은 선의 개수를 늘리는 효과**  \n","→ 학습시간은 더 걸리더라도 안정적인 성능을 낼 수 있음"],"metadata":{"id":"XXYkXfH2vdGN"}},{"cell_type":"code","source":["model = nn.Sequential(\n","    nn.Linear(2, 10, bias=True), nn.Sigmoid(),\n","    nn.Linear(2, 1, bias=True), nn.Sigmoid()).to(device)"],"metadata":{"id":"rCTNwu1BtDJ7","executionInfo":{"status":"ok","timestamp":1728035267028,"user_tz":-540,"elapsed":755,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"mpng9UzPtesB"}},{"cell_type":"markdown","source":["# Shallow ANN"],"metadata":{"id":"clXGck4DuYnA"}},{"cell_type":"markdown","source":["Hidden layer를 없애고 Single-layer Perceptron으로 변경  \n","single layer perceptron은 linear separable problem만을 풀 수 있기 때문에 non-linear separable problem은 풀 수 없음  \n","→ 성능 떨어짐"],"metadata":{"id":"D4t94G_uvLZ5"}},{"cell_type":"code","source":["model = nn.Sequential(\n","    nn.Linear(2 , 1, bias=True), nn.Sigmoid()).to(device)"],"metadata":{"id":"9frUl3ruuax2","executionInfo":{"status":"ok","timestamp":1728035267028,"user_tz":-540,"elapsed":6,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"ZZ-0Lpqpv6d2"}},{"cell_type":"markdown","source":["# Deep ANN"],"metadata":{"id":"ywfTVRGIwPta"}},{"cell_type":"markdown","source":["hidden layer 층을 1개에서 2개로 변경  \n","**deeping은 선을 구부리는 효과**"],"metadata":{"id":"D9kQ0r46wSfS"}},{"cell_type":"code","source":["model = nn.Sequential(\n","    nn.Linear(2, 2, bias=True), nn.Sigmoid(),\n","    nn.Linear(2, 2, bias=True), nn.Sigmoid(),\n","    nn.Linear(2, 1, bias=True), nn.Sigmoid()).to(device)"],"metadata":{"id":"ztAZhFcDwRJm","executionInfo":{"status":"ok","timestamp":1728035267028,"user_tz":-540,"elapsed":5,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"nEr_x6SWw-rT"}},{"cell_type":"markdown","source":["# Deeper & Wider ANN"],"metadata":{"id":"wH5iAbWTxDMl"}},{"cell_type":"markdown","source":["hidden layer 층을 1개에서 7개로 변경  \n","Q. 과연 성능이 괜찮을까?  \n","A. 아니다! **Vanishing Gradient** 문제  \n","→ 활성함수를 sigmoid에서 ReLU를 사용함으로써 해결됨"],"metadata":{"id":"LzPy_o4vxEZX"}},{"cell_type":"code","source":["# sigmoid as activation function\n","model_org = nn.Sequential(\n","    nn.Linear(2, 10, bias=True), nn.Sigmoid(),\n","    nn.Linear(10, 10, bias=True), nn.Sigmoid(),\n","    nn.Linear(10, 10, bias=True), nn.Sigmoid(),\n","    nn.Linear(10, 10, bias=True), nn.Sigmoid(),\n","    nn.Linear(10, 10, bias=True), nn.Sigmoid(),\n","    nn.Linear(10, 10, bias=True), nn.Sigmoid(),\n","    nn.Linear(10, 10, bias=True), nn.Sigmoid(),\n","    nn.Linear(10, 1, bias=True), nn.Sigmoid()).to(device)\n","\n","# ReLU as activation function\n","\n","''' 마지막 층이 nn.Sigmoid()인 이유\n","    : classification 문제를 풀고 있기 때문에 마지막 output은 0과 1 사이의 값으로 바꾸기 위함\n","'''\n","model_new = nn.Sequential(\n","    nn.Linear(2, 10, bias=True), nn.ReLU(),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(),\n","    nn.Linear(10, 1, bias=True), nn.Sigmoid()).to(device)"],"metadata":{"id":"OIQDYbMlxHmL","executionInfo":{"status":"ok","timestamp":1728035267028,"user_tz":-540,"elapsed":4,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"KZVW9tbxxaHr"}},{"cell_type":"markdown","source":["# Dropout"],"metadata":{"id":"8apNrrEgyl5B"}},{"cell_type":"markdown","source":["**학습 과정**중에 지정된 비율로 임의의 연결을 끊음으로써 일반화 성능을 개선하는 방법"],"metadata":{"id":"s_W1EnnO5XVz"}},{"cell_type":"code","source":["model = nn.Sequential(\n","    nn.Linear(2, 10, bias=True), nn.ReLU(), nn.Dropout(0.1),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(), nn.Dropout(0.1),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(), nn.Dropout(0.1),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(), nn.Dropout(0.1),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(), nn.Dropout(0.1),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(), nn.Dropout(0.1),\n","    nn.Linear(10, 10, bias=True), nn.ReLU(), nn.Dropout(0.1),\n","    nn.Linear(10, 1, bias=True), nn.Sigmoid()).to(device)"],"metadata":{"id":"XByIkTpT5USw","executionInfo":{"status":"ok","timestamp":1728035267028,"user_tz":-540,"elapsed":3,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 평가 모드 시에는 학습 시에 적용했던 드롭 아웃 여부 등을 비적용\n","model.eval()\n","with torch.no_grad():\n","    hypothesis = model(input_features)"],"metadata":{"id":"8cRj17FL6DvD","executionInfo":{"status":"ok","timestamp":1728035267028,"user_tz":-540,"elapsed":3,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":12,"outputs":[]}]}