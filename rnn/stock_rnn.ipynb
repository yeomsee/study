{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1WoAhwkRZHpKXF5v9kqjiA9NnEDYFQson","authorship_tag":"ABX9TyPIdsjdzSyqYiOV4PgjF08C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"iY_l4Nhb5m-8","executionInfo":{"status":"ok","timestamp":1728923786542,"user_tz":-540,"elapsed":5187,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import (DataLoader, RandomSampler, TensorDataset)\n","import csv\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","source":["class STOCK_RNN(nn.Module):\n","    # 생성자 오버라이드\n","    def __init__(self, config):\n","        super(STOCK_RNN, self).__init__()\n","\n","        self.input_size = config['input_size']\n","        self.hidden_size = config['hidden_size']\n","        self.output_size = config['output_size']\n","        self.num_layers = config['num_layers']\n","        self.batch_size = config['batch_size']\n","\n","        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, bidirectional=False, batch_first=True)\n","\n","        ''' self.num_layers\n","            ㅁ-ㅁ-ㅁ-...-ㅁ -> 이런 층이 몇 개있는가?\n","        '''\n","\n","        ''' batch_first = True\n","            batch에 들어오는 순서대로 처리\n","        '''\n","\n","        self.linear = nn.Linear(self.hidden_size, self.output_size)\n","\n","    # forward 오버라이드 -> hypothesis 형성\n","    def forward(self, input_features):\n","\n","        x, (h_n, c_n) = self.lstm(input_features)\n","\n","        ''' lstm의 return\n","            output, (last hidden state, last cell state)\n","\n","            - output : (batch, sequence, hidden)\n","        '''\n","\n","        # output에서 맨 마지막 시퀀스의 (batch, hidden) 정보를 가져옴\n","        h_t = x[:, -1, :]\n","\n","        hypothesis = self.linear(h_t)\n","\n","        return hypothesis"],"metadata":{"id":"V7S6Qd8qaPiM","executionInfo":{"status":"ok","timestamp":1728923786542,"user_tz":-540,"elapsed":3,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def load_dataset(file_name):\n","\n","    f = open(file_name, 'r', encoding='cp949')\n","\n","    data = csv.reader(f, delimiter=',')\n","\n","    # header 건너뛰기\n","    next(data)\n","\n","    data_X, data_Y = [], []\n","\n","    for row in data:\n","        # 오픈, 공가, 저가, 거래량\n","        data_X.append([float(i) for i in row[2:]])\n","\n","        # 종가\n","        data_Y.append(float(row[1]))\n","\n","    # data 범위가 매우 상이하기 때문에 MinMax '정규화' 적용 (단, 종속변수는 제외)\n","    scaler = MinMaxScaler()\n","    scaler.fit(data_X)\n","    data_X = scaler.transform(data_X)\n","\n","    data_num = len(data_X)\n","    sequence_len = config['sequence_len']\n","    seq_data_X, seq_data_Y = [], []\n","\n","    # window 크기만큼 슬라이딩 하면서 데이터 생성 : 원본 데이터의 시퀀스화\n","    for i in range(data_num - sequence_len):\n","        window_size = i + sequence_len\n","        seq_data_X.append(data_X[i:window_size])\n","        seq_data_Y.append([data_Y[window_size-1]])\n","\n","    ''' 현재 우리가 하려는 건 매 시퀀스마다 output을 내는 과정이 아님\n","\n","        예를 들어,\n","        [[1,2], [3,4], [5,6], [7,8]]와 같이 입력데이터가 구성돼있으면,\n","        [[1,2], [3,4], [5,6]]이 들어왔을 때 다음 예측, [[3,4], [5,6], [7,8]]이 들어왔을 때 다음 예측\n","\n","        때문에 입력 데이터를 window size만큼 잘라서 구성해야함\n","        [[1,2], [3,4], [5,6], [7,8]]\n","        ->[[[1,2], [3,4], [5,6]], [[3,4], [5,6], [7,8]]]\n","    '''\n","\n","    (train_X, train_Y) = (np.array(seq_data_X[:]), np.array(seq_data_Y[:]))\n","    train_X = torch.tensor(train_X, dtype=torch.float)\n","    train_Y = torch.tensor(train_Y, dtype=torch.float)\n","\n","    print(train_X.shape) # (73, 3, 4)\n","    print(train_Y.shape) # (73, 1)\n","\n","    return (train_X, train_Y)"],"metadata":{"id":"COGrpErLbP1J","executionInfo":{"status":"ok","timestamp":1728923786542,"user_tz":-540,"elapsed":3,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()"],"metadata":{"id":"EsdQKuXDxwBJ","executionInfo":{"status":"ok","timestamp":1728923786542,"user_tz":-540,"elapsed":3,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def do_test(model, test_dataloader):\n","    model.eval()\n","\n","    predicts, golds = [], []\n","\n","    with torch.no_grad():\n","        for step, batch in enumerate(test_dataloader):\n","            batch = tuple(t.cuda() for t in batch)\n","\n","            input_features, labels = batch\n","            hypothesis = model(input_features)\n","\n","            # logits = (hypothesis > 0.5).float()\n","            '''\n","                우리가 지금 하려는 거는 classification이 아닌 regression이므로 argmax는 필요없다\n","            '''\n","\n","            x = tensor2list(hypothesis[:, 0])\n","            y = tensor2list(labels)\n","\n","            predicts.extend(x)\n","            golds.extend(y)\n","\n","    # 소수점 이하 1자리로 변환\n","    predicts = [round(i, 1) for i in predicts]\n","    golds = [round(i[0], 1) for i in golds]\n","\n","    print(\"pred = \", predicts)\n","    print(\"gold = \", golds)"],"metadata":{"id":"ZiBva8IHwHJm","executionInfo":{"status":"ok","timestamp":1728923786542,"user_tz":-540,"elapsed":2,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def test(config):\n","    model = STOCK_RNN(config).cuda()\n","    model.load_state_dict(torch.load(os.path.join['output_dir'], config['model_name']))\n","\n","    features, labels = load_dataset(config['file_name'])\n","    test_features = TensorDataset(features, labels)\n","    test_dataloader = DataLoader(test_features, shuffle=True, batch_size=config['batch_size'])\n","\n","    do_test(model, test_dataloader)"],"metadata":{"id":"ov_qykWSx91F","executionInfo":{"status":"ok","timestamp":1728923786542,"user_tz":-540,"elapsed":2,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def train(config):\n","    model = STOCK_RNN(config).cuda()\n","\n","    (input_features, labels) = load_dataset(config['file_name'])\n","    tensor_features = TensorDataset(input_features, labels)\n","    train_dataloader = DataLoader(tensor_features, shuffle=True, batch_size=config['batch_size'])\n","\n","    loss_func = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config['learn_rate'])\n","\n","    for epoch in range(config['epoch']+1):\n","        model.train()\n","\n","        costs = []\n","\n","        for step, batch in enumerate(train_dataloader):\n","            batch = tuple(t.cuda() for t in batch)\n","\n","            input_features, labels = batch\n","\n","            optimizer.zero_grad()\n","\n","            hypothesis = model(input_features)\n","\n","            cost = loss_func(hypothesis, labels)\n","\n","            cost.backward()\n","\n","            optimizer.step()\n","\n","            costs.append(cost.data.item())\n","\n","        print(f\"Average Loss = {0:f}\".format(np.mean(costs)))\n","        torch.save(model.state_dict(), os.path.join(config['output_dir'], 'epoch_{0:d}.pt').format(epoch))\n","\n","        do_test(model, train_dataloader)"],"metadata":{"id":"ao1R33uSzIwx","executionInfo":{"status":"ok","timestamp":1728923786542,"user_tz":-540,"elapsed":2,"user":{"displayName":"염시형","userId":"10102578423005698819"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","    root_dir = \"/content/drive/MyDrive/연구실/기계학습/rnn\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    config = {\"mode\": \"train\",\n","            \"model_name\":\"epoch_{0:d}.pt\".format(10),\n","            \"output_dir\":output_dir,\n","            \"file_name\": os.path.join(root_dir, \"samsung-2020.csv\"),\n","            \"sequence_len\": 3,\n","            \"input_size\": 4,\n","            \"hidden_size\": 10,\n","            \"output_size\": 1,\n","            \"num_layers\": 1,\n","            \"batch_size\": 1,\n","            \"learn_rate\": 0.1,\n","            \"epoch\": 10,\n","            }\n","\n","    if config[\"mode\"] == \"train\":\n","        train(config)\n","    else:\n","        test(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HhtPpX440Y4p","executionInfo":{"status":"ok","timestamp":1728923792345,"user_tz":-540,"elapsed":5805,"user":{"displayName":"염시형","userId":"10102578423005698819"}},"outputId":"4d58fa72-63f4-4a98-9ce6-749bbc18be0e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([72, 3, 4])\n","torch.Size([72, 1])\n","Average Loss = 0.000000\n","pred =  [53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.0, 53.0, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.0, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.1, 53.0, 53.1, 53.1, 53.1, 53.1]\n","gold =  [54.2, 62.3, 56.4, 57.2, 60.7, 55.9, 45.4, 48.7, 61.3, 57.8, 60.8, 58.9, 60.5, 61.8, 59.7, 59.1, 60.2, 43.0, 59.0, 59.2, 54.6, 59.5, 55.5, 56.5, 59.2, 54.2, 48.3, 47.3, 47.8, 57.4, 56.8, 52.1, 57.9, 61.5, 48.3, 60.4, 55.4, 61.8, 57.2, 60.8, 60.0, 55.0, 42.5, 61.3, 59.5, 59.8, 45.4, 47.0, 56.4, 55.5, 59.9, 60.0, 56.5, 55.8, 60.4, 50.8, 61.4, 50.0, 56.8, 56.5, 61.1, 58.8, 54.2, 45.6, 60.7, 59.5, 62.4, 47.8, 48.9, 58.6, 50.0, 60.0]\n","Average Loss = 0.000000\n","pred =  [56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8, 56.8]\n","gold =  [55.5, 50.0, 57.8, 47.8, 55.5, 54.2, 50.8, 60.7, 60.0, 47.8, 61.4, 56.5, 57.2, 57.2, 59.2, 42.5, 60.4, 55.8, 57.9, 56.5, 61.5, 54.2, 61.1, 59.9, 60.0, 59.8, 55.9, 55.0, 60.4, 58.8, 59.2, 60.7, 59.1, 61.8, 59.0, 45.4, 61.8, 56.4, 58.9, 54.6, 52.1, 57.4, 47.3, 61.3, 45.6, 50.0, 59.5, 60.0, 56.8, 54.2, 59.5, 60.5, 48.3, 43.0, 47.0, 60.8, 56.5, 56.4, 59.5, 55.4, 60.8, 58.6, 60.2, 62.3, 48.3, 62.4, 45.4, 61.3, 59.7, 48.7, 48.9, 56.8]\n","Average Loss = 0.000000\n","pred =  [55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.1, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2, 55.2]\n","gold =  [62.4, 55.5, 48.3, 56.8, 62.3, 60.7, 59.7, 56.5, 56.8, 45.4, 55.8, 60.5, 57.8, 61.4, 47.3, 60.4, 58.6, 42.5, 61.3, 59.9, 54.2, 57.9, 56.4, 52.1, 59.2, 55.0, 57.2, 59.0, 59.1, 60.0, 45.4, 54.2, 47.0, 48.7, 45.6, 56.5, 60.7, 61.8, 61.3, 56.4, 55.4, 47.8, 57.4, 60.0, 60.4, 48.3, 50.8, 60.2, 61.8, 43.0, 54.6, 59.8, 50.0, 61.5, 57.2, 50.0, 58.9, 48.9, 47.8, 61.1, 55.5, 59.5, 59.2, 54.2, 59.5, 55.9, 60.8, 60.8, 59.5, 58.8, 60.0, 56.5]\n","Average Loss = 0.000000\n","pred =  [56.4, 56.4, 56.2, 56.2, 56.2, 56.3, 56.4, 56.3, 55.3, 56.4, 56.3, 54.1, 56.4, 56.2, 56.4, 55.3, 56.3, 56.4, 55.2, 56.2, 54.7, 56.4, 56.4, 56.4, 56.3, 56.2, 56.2, 56.1, 53.8, 56.4, 56.2, 56.4, 56.2, 56.4, 56.4, 53.0, 56.4, 56.1, 54.4, 56.4, 56.4, 54.4, 56.0, 55.9, 56.4, 55.8, 56.4, 56.3, 54.0, 56.4, 56.4, 56.0, 56.3, 56.4, 56.4, 56.2, 55.6, 54.9, 56.4, 56.2, 56.1, 53.1, 56.4, 56.4, 56.4, 56.4, 56.0, 56.2, 56.4, 56.4, 56.3, 55.2]\n","gold =  [60.0, 60.0, 55.8, 54.2, 59.5, 59.5, 61.3, 57.2, 48.3, 59.2, 56.5, 47.0, 61.4, 55.5, 59.0, 48.3, 54.6, 61.5, 50.0, 54.2, 48.9, 57.9, 62.4, 60.7, 58.9, 55.5, 56.5, 57.8, 45.4, 61.3, 55.9, 58.8, 59.5, 59.7, 60.4, 42.5, 59.9, 56.5, 43.0, 61.8, 60.0, 45.4, 52.1, 50.8, 59.1, 50.0, 56.8, 57.2, 47.8, 61.8, 59.2, 57.4, 56.4, 60.8, 62.3, 54.2, 47.3, 47.8, 56.4, 56.8, 55.4, 48.7, 59.8, 60.4, 60.7, 60.8, 55.0, 58.6, 60.2, 60.5, 61.1, 45.6]\n","Average Loss = 0.000000\n","pred =  [57.6, 57.6, 44.9, 57.6, 57.6, 57.6, 44.6, 57.6, 57.6, 57.6, 57.4, 57.4, 45.1, 57.1, 57.6, 56.8, 57.2, 57.0, 52.4, 57.5, 57.6, 57.2, 53.4, 57.6, 44.8, 44.6, 57.6, 57.6, 57.5, 57.6, 57.6, 57.6, 57.6, 57.5, 57.5, 57.1, 57.6, 44.7, 57.6, 48.2, 57.6, 56.8, 44.8, 57.6, 57.6, 57.6, 57.5, 57.0, 57.6, 57.6, 57.3, 57.6, 57.6, 44.6, 57.5, 57.6, 57.6, 45.2, 57.6, 45.2, 57.6, 57.5, 52.9, 57.3, 57.2, 57.2, 44.9, 51.4, 46.2, 57.4, 44.4, 44.6]\n","gold =  [60.7, 56.8, 47.8, 61.3, 58.8, 59.0, 48.9, 59.1, 61.5, 60.2, 56.5, 55.8, 48.3, 59.5, 61.3, 57.8, 54.2, 56.8, 52.1, 61.1, 61.4, 59.5, 55.0, 59.8, 45.4, 47.8, 61.8, 59.2, 54.6, 62.3, 60.8, 56.4, 57.9, 57.2, 58.9, 54.2, 59.7, 50.0, 62.4, 50.0, 60.0, 56.5, 47.0, 57.2, 60.4, 60.0, 59.5, 55.4, 60.5, 59.2, 55.9, 61.8, 60.4, 45.4, 56.4, 60.0, 60.8, 48.3, 60.7, 48.7, 59.9, 56.5, 57.4, 58.6, 55.5, 55.5, 45.6, 50.8, 47.3, 54.2, 43.0, 42.5]\n","Average Loss = 0.000000\n","pred =  [58.8, 58.9, 58.8, 58.4, 57.9, 58.9, 58.9, 58.9, 58.9, 58.9, 58.9, 46.9, 54.3, 47.8, 58.8, 58.9, 58.9, 58.9, 58.9, 58.9, 58.9, 58.3, 58.8, 47.9, 58.9, 46.5, 58.9, 46.3, 58.9, 58.9, 58.9, 58.9, 58.9, 58.9, 58.9, 48.0, 45.6, 46.5, 58.9, 58.9, 58.9, 47.8, 58.5, 58.8, 58.9, 58.9, 58.9, 58.9, 58.9, 58.9, 58.9, 58.9, 58.8, 58.9, 58.9, 58.9, 58.9, 58.8, 58.9, 58.9, 58.8, 46.4, 58.9, 58.9, 49.1, 58.9, 45.7, 58.9, 58.9, 58.9, 45.9, 45.7]\n","gold =  [57.8, 56.5, 55.4, 57.4, 50.8, 61.3, 60.7, 59.5, 59.9, 60.4, 56.4, 48.7, 50.0, 50.0, 59.5, 56.4, 60.0, 60.7, 59.2, 60.8, 60.0, 52.1, 55.5, 48.3, 55.5, 47.8, 62.3, 45.4, 61.3, 54.2, 57.9, 60.0, 59.8, 61.4, 59.5, 48.3, 43.0, 47.0, 61.1, 61.8, 60.2, 45.6, 55.0, 54.2, 58.6, 62.4, 58.8, 60.8, 58.9, 57.2, 55.8, 60.5, 56.5, 56.5, 59.1, 60.4, 55.9, 54.2, 59.0, 57.2, 56.8, 47.8, 61.5, 61.8, 47.3, 59.2, 45.4, 54.6, 56.8, 59.7, 48.9, 42.5]\n","Average Loss = 0.000000\n","pred =  [58.5, 57.9, 45.8, 58.5, 57.9, 58.5, 58.4, 58.5, 58.5, 57.8, 58.5, 57.0, 57.6, 58.5, 58.5, 58.2, 57.5, 57.5, 58.5, 58.4, 57.3, 58.5, 57.0, 57.7, 45.5, 46.9, 58.5, 58.5, 58.6, 58.5, 58.5, 57.3, 58.5, 47.2, 57.1, 45.6, 45.9, 58.6, 47.8, 58.5, 46.4, 58.5, 57.7, 46.5, 58.5, 58.5, 45.6, 58.3, 57.7, 58.4, 58.5, 58.6, 46.1, 49.9, 58.5, 58.5, 58.6, 58.0, 58.1, 45.6, 58.5, 58.0, 57.8, 58.3, 56.9, 45.6, 57.5, 55.9, 58.5, 58.5, 57.7, 58.1]\n","gold =  [59.7, 56.5, 47.8, 60.2, 55.4, 59.2, 54.6, 60.4, 60.8, 56.5, 61.3, 52.1, 59.5, 60.7, 59.2, 56.4, 59.5, 54.2, 58.8, 58.9, 57.8, 60.8, 57.4, 55.9, 42.5, 45.6, 62.3, 60.0, 61.4, 61.8, 60.4, 56.5, 56.4, 48.3, 55.0, 43.0, 47.8, 59.9, 50.0, 56.8, 48.3, 57.9, 56.8, 48.9, 62.4, 60.0, 47.0, 57.2, 55.5, 60.0, 61.3, 59.1, 48.7, 47.3, 60.5, 61.5, 59.8, 58.6, 59.5, 45.4, 61.8, 61.1, 54.2, 55.8, 50.8, 45.4, 54.2, 50.0, 59.0, 60.7, 55.5, 57.2]\n","Average Loss = 0.000000\n","pred =  [48.1, 59.5, 59.5, 56.2, 58.3, 59.4, 59.3, 47.6, 48.1, 56.8, 52.5, 59.6, 58.5, 58.6, 59.0, 52.3, 57.6, 59.3, 58.9, 54.5, 59.6, 58.0, 59.6, 56.6, 46.8, 59.6, 59.6, 47.8, 49.1, 59.6, 58.4, 55.5, 57.0, 59.6, 59.6, 47.6, 46.7, 47.4, 59.5, 59.5, 47.6, 59.6, 59.6, 46.7, 58.8, 59.5, 48.1, 59.3, 59.4, 59.6, 51.0, 58.6, 59.5, 56.2, 46.6, 47.0, 55.8, 59.6, 59.5, 59.6, 56.8, 59.6, 59.6, 59.6, 47.8, 58.1, 59.6, 59.5, 47.8, 59.6, 57.5, 59.4]\n","gold =  [47.3, 59.2, 60.7, 54.2, 58.6, 56.4, 54.6, 47.8, 48.9, 59.5, 57.4, 61.4, 57.2, 61.1, 57.2, 55.0, 55.9, 60.0, 55.8, 57.8, 59.1, 54.2, 57.9, 56.8, 42.5, 60.0, 61.8, 48.3, 50.8, 59.8, 56.5, 54.2, 55.5, 62.3, 62.4, 48.7, 47.0, 43.0, 60.0, 60.5, 45.6, 58.8, 61.8, 47.8, 56.4, 60.4, 50.0, 58.9, 56.8, 59.9, 52.1, 59.5, 61.3, 55.4, 45.4, 45.4, 56.5, 60.8, 61.3, 59.2, 55.5, 60.2, 60.7, 61.5, 48.3, 56.5, 60.8, 59.0, 50.0, 59.7, 59.5, 60.4]\n","Average Loss = 0.000000\n","pred =  [39.9, 58.7, 58.4, 58.7, 47.0, 58.8, 58.0, 58.5, 58.7, 45.2, 57.3, 45.6, 58.5, 57.5, 45.3, 59.0, 58.4, 58.7, 58.4, 47.6, 58.8, 58.1, 58.7, 44.8, 58.9, 45.3, 58.7, 46.4, 46.3, 58.7, 58.7, 58.6, 52.6, 58.7, 58.6, 58.8, 58.7, 58.6, 58.7, 46.3, 58.8, 57.5, 58.1, 58.7, 45.3, 58.7, 58.7, 58.0, 49.1, 51.1, 58.4, 47.2, 46.1, 58.7, 58.3, 50.4, 58.2, 58.8, 58.6, 58.4, 58.0, 58.4, 58.7, 58.2, 58.6, 57.6, 58.1, 58.1, 45.8, 58.7, 58.7, 58.8]\n","gold =  [45.4, 62.4, 61.1, 60.5, 50.0, 60.2, 59.5, 57.2, 60.4, 48.9, 57.8, 50.0, 58.6, 54.2, 47.8, 59.8, 56.4, 61.3, 59.5, 47.3, 59.1, 54.2, 61.8, 43.0, 61.4, 45.6, 61.8, 48.3, 47.8, 59.7, 61.3, 60.0, 55.0, 58.8, 58.9, 62.3, 59.2, 56.4, 61.5, 48.3, 60.0, 56.5, 56.5, 57.9, 45.4, 60.0, 60.8, 55.9, 50.8, 57.4, 54.6, 48.7, 47.0, 60.7, 57.2, 52.1, 56.5, 60.8, 60.4, 55.8, 55.5, 55.4, 60.7, 56.8, 56.8, 54.2, 59.5, 55.5, 42.5, 59.0, 59.2, 59.9]\n","Average Loss = 0.000000\n","pred =  [46.5, 59.1, 48.2, 58.8, 59.0, 46.6, 59.0, 57.0, 42.5, 59.2, 56.9, 57.4, 46.6, 57.7, 60.1, 53.3, 58.9, 60.3, 59.1, 59.3, 56.9, 46.6, 46.0, 57.1, 57.9, 58.6, 60.3, 57.3, 59.2, 57.6, 48.5, 58.6, 59.4, 58.4, 57.8, 60.0, 56.7, 60.3, 57.4, 59.4, 45.6, 45.4, 58.6, 59.8, 45.8, 56.8, 54.5, 57.2, 58.9, 56.8, 45.5, 57.5, 59.4, 59.1, 60.2, 58.4, 59.1, 45.5, 57.7, 45.9, 56.7, 56.5, 46.5, 57.1, 58.2, 59.3, 59.1, 57.1, 55.6, 55.1, 59.7, 59.9]\n","gold =  [45.6, 60.4, 47.3, 61.3, 60.0, 50.0, 60.7, 55.9, 45.4, 61.8, 57.8, 59.5, 48.7, 59.5, 60.0, 50.8, 61.3, 61.4, 61.8, 60.8, 56.5, 48.3, 47.8, 56.8, 57.2, 60.4, 59.1, 56.4, 59.2, 58.6, 50.0, 56.8, 62.4, 60.0, 55.8, 57.9, 55.5, 59.8, 59.5, 61.5, 47.0, 45.4, 58.9, 60.8, 48.9, 55.5, 52.1, 57.2, 60.5, 54.2, 43.0, 55.4, 58.8, 59.0, 59.9, 56.4, 59.2, 42.5, 61.1, 47.8, 54.2, 54.2, 48.3, 56.5, 54.6, 60.7, 59.7, 56.5, 55.0, 57.4, 62.3, 60.2]\n","Average Loss = 0.000000\n","pred =  [59.2, 59.1, 59.2, 59.7, 45.8, 55.8, 46.8, 55.6, 58.8, 43.8, 59.2, 56.1, 55.8, 56.4, 55.5, 59.7, 59.3, 59.6, 59.6, 52.6, 56.5, 59.6, 45.6, 59.5, 59.7, 59.1, 59.2, 56.2, 59.2, 59.3, 50.6, 56.1, 58.6, 58.4, 56.0, 45.3, 55.4, 52.1, 55.7, 57.0, 45.5, 57.7, 59.7, 57.4, 59.2, 56.6, 56.4, 55.6, 59.4, 56.3, 59.4, 56.2, 58.5, 56.8, 55.9, 46.3, 45.3, 59.5, 57.1, 59.6, 59.5, 57.2, 59.6, 45.9, 58.0, 57.1, 55.5, 58.7, 59.3, 59.0, 50.8, 56.1]\n","gold =  [60.7, 61.3, 60.4, 59.1, 47.8, 54.2, 48.7, 52.1, 56.8, 45.4, 59.2, 55.9, 54.2, 59.5, 50.0, 61.4, 59.0, 57.9, 60.2, 48.3, 59.5, 60.8, 47.0, 61.5, 59.8, 60.5, 60.0, 56.5, 61.8, 61.8, 45.6, 54.2, 56.4, 58.9, 57.8, 45.4, 47.3, 48.3, 57.4, 55.4, 43.0, 57.2, 59.9, 55.8, 59.7, 57.2, 56.8, 55.0, 60.7, 56.5, 60.8, 55.5, 60.0, 56.4, 56.5, 47.8, 42.5, 58.8, 58.6, 62.3, 62.4, 61.1, 60.0, 48.9, 54.6, 59.5, 50.8, 60.4, 59.2, 61.3, 50.0, 55.5]\n"]}]}]}
